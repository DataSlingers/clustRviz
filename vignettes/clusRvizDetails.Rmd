---
title: "clustRviz Details"
author: "John Nagorski"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    css: style.css
vignette: >
  %\VignetteIndexEntry{clustRviz Details}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval=TRUE,
  message = FALSE
)
```

\renewcommand{\vec}[1]{\boldsymbol{#1}}

# Introduction

The `clustRviz` package intends to make fitting and visualizing CARP and CBASS 
solution paths an easy process. In the [Getting Started](Getting_Started.html) 
vignettee we provide a quick start guide for basic usage, fitting, 
and plotting. In this vignette, we build on the basics and provide a more 
detailed explanation for the variety of options available in `clustRviz`.



# Background

The starting point for CARP the Convex Clustering problem. 


$$
\underset{\vec U}{\textrm{minimize}} \;\;
\frac{1}{2} \| \vec X - \vec U \|_F^2 + 
\lambda \sum_{ l < m} 
w_{l,m} \| \vec u_l - \vec u_m \|_2
$$


The above can be solved via ADMM

$$
\underset{\vec U, \vec V}{\text{minimize}}\;\;
\frac{1}{2} \| \vec X - \vec U \|_2^2  +  \lambda \sum_{l<m} w_{l,m} \|  \vec v_{l,m} \|_2 \\
\text{subject to}\;\;\;\;\;
\vec u_{l} - \vec u_{m} = \vec v_{l,m}, \;\; \forall l,m
$$

Convex clustering is traditionally solved by iteratively applying ADMM updates until convergence.

This must be done for each $\lambda_k$ which is costly.



# Preprocessing and Inputs

While the `CARP` and `CBASS` functions provides several reasonable default choices for weights, algorithms, etc, it
is important to know their details if one wishes to compute more customized clustering choices.
Here we examine several of the inputs to `CARP` and `CBASS`, as well as their preprocessing technqiues 

While we will still use the `CARP` and `CBASS` functions, we will also encounter serveral other
package functions along the way.

As before, we will use the presidential speech dataset to illustrate 
the variety of options available. Let's begin by loading our
package and the dataset.

```{r}
library(clustRviz)
data("presidential_speech")
Xdat <- presidential_speech
obs.labels <- rownames(Xdat)
var.labels <- colnames(Xdat)
Xdat[1:5,1:5]
head(obs.labels)
head(var.labels)
```


Again our speech data consists of log-transformed word counts from speech found at url.

## Preprocessing 

### Normalization
An important first choice before clustering is whether to center and scale our observations.

Centering should always be done.

The choice of scaling is left to the user discression, but should typically be applied 
if measurements are a vastly different scales.

In the case of the presidental speech dataset, all variables are of the same type and 
so we do not scale our data matrix

```{r}
# Centering data before computing the CARP solution path
Xdat.preprocessed <- scale(Xdat,center=TRUE,scale=FALSE)
```

In the `CARP` function this preprocessing is done via the `X.center` and `X.scale`
arguements.
If the data is pre-processed outside of `CARP`, as is the case here, these options
may be set to `FALSE`

Similarly, the `CBASS` function also requires that data preprocessed prior usage.
Because `CBASS` clusters both observations and variables, here centering is done 
by subtracting away the global mean of our data matrix.

```{r}
# Subtracting global mean before computing the CBASS solution path
Xdat.bi <- Xdat
Xdat.bi.preprocessed <- Xdat -  mean(Xdat)
```

### Dimension Reduction

While not directly addressed by `CARP` or `CBASS`, high dimensional 
measurements can present a challenge for clustering methods.

Owing to the "curse of dimensionality", high dimensional measurements 
may deliver sub-optimal performance for distance-based methods generally.

As such, performing dimensionality reduction before applying `CARP`  
may result in more interpretable clusters.

We leave the choice of dimensionality reduction to the end-user, 
but still recommend the reduced feature set be pre-processed 
as described above.

For the purpose of visualuization, `CARP` addresses the problem 
of high dimenstionality by visualizing the principal components 
of the data by default.


## Weights

Weights are arguably one of the most important inputs to both `CARP` and `CBASS`.

As we can see from the optimization probelms above, weights impart apriori 
preference concerning which observations (and variables in the case of `CBASS`)
should be fused together.

This important feature allows us to incorportate domain knowledge, if available.

In the absense of domain knowledge, however, weights can easily be 
constructed via the data itself.

One might first consider simply using a set of uninformative weights,
$w_{l.m} = 1$

While perhaps not appearent at first, such weight choices can also greatly 
affect the computation time of both `CARP` and `CBASS` 
(and certainly Convex Clustering).

To see this note that in the Convex Clustering problem above 
the regularization term contains $\binom{n}{2}$ summands, 
drastically increasing computations as $n$ increases.

Computations can be reduced by making the majority of weights zero and 
greatly reducing the number of summands.

Here we detail `clustRviz`'s default weight choices, as well as how 
weights can be constructed manually.

By default `CARP` and `CBASS` assume no prior domain knowledge 
and construct distance-based weights via a gaussian kernel:

$$
w_{l,m}  = \exp\{ -\phi \|\vec x_l - \vec x_m \|^2_2 \}
$$
Given a choice of $\phi$ (discussed in more detail below), an initial set of 
weights can be constructed via `DenseWeights` function. In the case of presidential
speech data for example,

```{r}
dense.weights <- clustRviz:::DenseWeights(X = Xdat.preprocessed,phi=1e-3)
head(dense.weights)
```

As the name suggests `DenseWeights` computes $w_{l,m}$ for all pairs of observations.

For the presidents data, with $n=44$ observations, this results in a vector of length 
$\binom{44}{2} = 956$.

```{r}
length(dense.weights)
```

The weights are returned in lexicographical order, namely 
$\vec w = (w_{1,2},w_{1,3},\dots,w_{l,m},\dots,w_{n-1,n} )$

The value of $\phi$ can have a marked effect on the weights.

```{r, fig.height=5,fig.width=10,fig.align='center',echo=FALSE,fig.cap="Dense weights for different choices of phi"}
library(tidyverse)
library(gridExtra)
gridExtra::grid.arrange(
clustRviz:::DenseWeights(X = Xdat.preprocessed,phi=1e-3) %>%
  qplot(bins=35)+
  xlab('Weight Value')+
  ylab('Count'),
clustRviz:::DenseWeights(X = Xdat.preprocessed,phi=1e-1) %>%
  qplot(bins=35) +
  xlab('Weight Value')+
  ylab('Count'),
ncol=2
)
```

The choice of $\phi$ determines variance of the weights.

Generally, choices of $\phi$ which result in higher variance weights tend to be more 
informative. A simple method for finding a reasonable choice of $\phi$ is a basic
grid search. For example:
```{r}
phi.vec <- 10^(-4:4)
sapply(phi.vec,function(phi){
  var(clustRviz:::DenseWeights(X=Xdat.preprocessed,phi = phi))
}) %>%
  which.max() %>%
  phi.vec[.]
```

Additionally, `DenseWeights` allows for the specification of alternative distance measures

Depending on the dataset, alternative metrics to traditional euclidean distance may 
result in more informative clustering solutions.

```{r}
dense.weights.can <- clustRviz:::DenseWeights(X = Xdat.preprocessed,phi=1e-5,method = 'canberra')
```


```{r, fig.height=5,fig.width=7,fig.align='center',echo=FALSE,fig.cap="Dense weights with Canberra distance"}
qplot(dense.weights.can,bins=35) + 
  xlab('Weight Value') +
  ylab('Count')
```

Distance-based weights can be made sparse by only fusing together nearest neighbors.

For a given observations $l$ we consider its $k$ nearest neighbors $\textrm{ne}_k(l)$. 

Given a pair of observations $l,m$, we set their corresponding weight, $w_{l,m}$, to zero 
if either is not amongst the other nearest neighbors. Specifically,

$$
\tilde{w}_{l,m} = w_{l,m} \vec{1}_{\textrm{ne}_k(l)}(m) \vec{1}_{\textrm{ne}_k(m)}(l)
$$
where $w_{l,m}$ is our original dense weight values and $\vec{1}_{A}(x)$ denotes the indicator function of the set $A$.

For a specifid value of $k$, a sparse vector of $k$ nearest neighbor weights can be generated via the `SparseWeights` function:

```{r}
k.neighbors <- 5
clustRviz:::SparseWeights(
  X=Xdat.preprocessed,
  dense.weights = dense.weights,
  k = k.neighbors) -> sparse.weights
```
In the case of $k=$ `r k.neighbors` above, the number of non-zero weight values 
(and hence the number of summands in our original objective) has been reduced from $\binom{44}{2} = 956$ 
to 

```{r}
sum(sparse.weights!=0)
```

Computationally, it is tempting to set $k$ small and ensure fast compuatation.

Setting $k$ too small, however, can potentially prevent observations from fusing, 
regardless of the amount of regularization.

In order to verify that our sparse set of weights allows for the eventual fusion 
of all observations we can examine its graphical representation.

The function `PlotWeightGraph` displays the fusion graph of our observations induced 
by the support of our weight set. 

The fusion graph displays each observation as a vertex, and connects 
two observation with an edge if their corresponding weight is non-zero. 

Such connections between observations indicate their ability fuse into a common 
cluster, given large enough regularization.

Below we consider the extreme case of $k=1$, which while computationally fast 
presents an immediate difficulty.

```{r,fig.width=5,fig.height=5,fig.align='center'}
clustRviz:::PlotWeightGraph(
  weights = clustRviz:::SparseWeights(
    X=Xdat.preprocessed,
    dense.weights = dense.weights,
    k=1
  ),
  nobs = nrow(Xdat.preprocessed),
  edge.labels = TRUE,
  obs.labels = obs.labels,
  vsize=3,
  label.scale=FALSE,
  label.cex=.7,
  repulsion=.9)
```

Examining the graph above, we note that the number of connected components is strictly greater than one.
As such, regardless of the number of `CARP` iteratations (and the associated regularization increases) it will 
not be possible fuse all observations into a single cluster.
Such extreme weight choices should typically be avoided as they not only hinder the agglomorative nature of `CARP` and `CBASS`, but also lead to biased and less interpretable solution paths. 
result in. 

At the other extreme, large values of $k$ again result in long computation time and 
bring little to no improvement on the resulting solution sequence; an example of 
such an overly dense graph can be seen below.

```{r,fig.width=5,fig.height=5,fig.align='center'}
clustRviz:::PlotWeightGraph(
  weights = clustRviz:::SparseWeights(
    X=Xdat.preprocessed,
    dense.weights = dense.weights,
    k=15
  ),
  nobs = nrow(Xdat.preprocessed),
  edge.labels = FALSE,
  obs.labels = obs.labels,
  vsize=3,
  label.scale=FALSE,
  label.cex=.7,
  repulsion=.9)
```

In order to provide a reasonable choice regarding weight sparsity, the function `MinKNN` may be used to 
find a balance between computational efficiency and poor clustering performance. 
`MinKNN` takes as its arguement an initial set of dense weights and returns the smallest $k$ such 
the graph induced by the support of the resulting set of sparse weights contains exactly one 
connected component. For our current vector of weights we have:
```{r}
clustRviz:::MinKNN(
  X = Xdat.preprocessed,
  dense.weights = dense.weights
) -> k.min
k.min
```
Using the `k.min` value above we may compute the corresponding sparse weight vector and 
view its resulting graph below.
```{r}
clustRviz:::SparseWeights(
  X = Xdat.preprocessed,
  dense.weights = dense.weights,
  k = k.min
) -> sparse.weights
```

```{r,fig.width=5,fig.height=5,fig.align='center'}
clustRviz:::PlotWeightGraph(
  weights = sparse.weights,
  nobs = nrow(Xdat.preprocessed),
  edge.labels = FALSE,
  obs.labels = obs.labels,
  vsize=3,
  label.scale=FALSE,
  label.cex=.7,
  repulsion=.9)
```

We can see that the resulting graph offers a compromise between the two extremes.
By default both `CARP` and `CBASS` use the value returned by `MinKNN` to produce 
sparse sets of weights. While slightly larger values of $k$ can be used, it is
not recommended to pass below this threshold for the reasons stated above.

# Fitting

`clustRviz` aims to make it easy to compute the CARP and CBASS solution paths, and 
to quickly begin exploring the results. To this end, many reasonable choices 
regarding both preprocessing and inputs disucussed above are made by default, 
allowing for solution path to be computed from the raw data alone. 
In the case of CARP, for example, we may fit the compute the solution 
path for the presidents data via the `CARP` function:
```{r}
CARP(
  X=Xdat,
  obs.labels = obs.labels
) -> carp.fit
```
Once completed, we can examine a brief summary of the fitted object
```{r,message=TRUE}
print(carp.fit)
```
The output above displays characteristics of our data, such as sample size 
and number of variables, and also gives a preview of the raw input. 
Additionally, the summary provides information regarding both data
pre-processing and weight computations. From the above we see that 
`CARP` has by default: (i) centered our data, (ii) computed 
distance-based weights using a gaussian kernel with $\phi=.001$, and 
(iii) created a sparse set of weights using $k=4$ nearest neighbors.
Finally, the summary also provides information about the algorithm, here 
CARP-VIZ, as well as the available visualizations.

While `CARP`'s default choices work well in most scenarios, customized inputs 
can also be provided. For example, choices regrarding preprocessing and weight 
computations discussed above can quickly be incorporated:
```{r,eval=FALSE}
CARP(
  X=Xdat,
  obs.labels = obs.labels,
  X.center=TRUE,
  X.scale=TRUE,
  phi = 1e-5,
  weight.dist = 'canberra',
  k = 5
) -> carp.fit.custom
```
Indeed, in the case strong apriori information concerning clusters is 
available, distance-based weight computations may be avoided altogether and 
user-specified weights given directly via the `weights` arguement.


`CBASS` solutions can be fit in a similar manner:

```{r}
CBASS(
  X=Xdat,
  obs.labels = obs.labels,
  var.labels = var.labels
) -> cbass.fit
```

And display its output:
```{r}
cbass.fit
```
# Solutions


```{r}
carp.clustering <- Clustering(carp.fit,k = 2)
```

```{r}
table(carp.clustering$clustering.assignment)
```

```{r,fig.width=5,fig.height=5,fig.align='center'}
clustRviz:::PlotWeightGraph(
  weights = sparse.weights,
  nobs = nrow(Xdat.preprocessed),
  edge.labels = FALSE,
  obs.labels = obs.labels,
  vsize=3,
  color=rainbow(2)[as.numeric(str_extract(carp.clustering$clustering.assignment,"(\\d)+"))],
  label.scale=FALSE,
  label.cex=.7,
  repulsion=.9)
```

```{r}
head(carp.clustering$cluster.means)
```


```{r,fig.width=5,fig.height=5,fig.align='center'}
carp.clustering <- Clustering(carp.fit,percent = .25)

clustRviz:::PlotWeightGraph(
  weights = sparse.weights,
  nobs = nrow(Xdat.preprocessed),
  edge.labels = FALSE,
  obs.labels = obs.labels,
  vsize=3,
  color=rainbow(ncol(carp.clustering$cluster.means))[
    as.numeric(str_extract(carp.clustering$clustering.assignment,"(\\d)+"))
    ],
  label.scale=FALSE,
  label.cex=.7,
  repulsion=.9)
```


```{r,eval=TRUE}
cbass.clustering <- Clustering(cbass.fit,percent = .8)
```




# Visualizations

## Static


```{r,fig.width=5,fig.height=5,fig.align='center'}
plot(carp.fit,type = 'path',percent=.5)
```

```{r,fig.width=5,fig.height=5,fig.align='center'}
plot(carp.fit,type = 'dendrogram')
```


## Dynamic

```{r,echo=FALSE,eval=TRUE,out.width='100%'}
knitr::include_url(url="https://clustrviz.shinyapps.io/PathShiny/",height='850px')
```


## Saving

```{r,echo=TRUE,eval=FALSE}
saveviz(
  carp.fit,
  file.name = 'path_dyn.gif',
  plot.type = 'path',
  image.type = 'dynamic'
)
```

```{r,echo=TRUE,eval=FALSE}
saveviz(
  carp.fit,
  file.name = 'carp_dend_static.png',
  plot.type = 'dendrogram',
  image.type = 'static'
)
```

```{r,echo=TRUE,eval=FALSE}
saveviz(
  cbass.fit,
  file.name = 'cbass_heat_dyn.gif',
  plot.type = 'heatmap',
  image.type = 'dynamic'
)

```
# Discussion




