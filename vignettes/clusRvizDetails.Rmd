---
title: "clustRviz Details"
author: "John Nagorski"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    css: style.css
vignette: >
  %\VignetteIndexEntry{clustRviz Details}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval=TRUE,
  message = FALSE
)
```

\renewcommand{\vec}[1]{\boldsymbol{#1}}
## Introduction

The `clustRviz` package intends to make fitting and visualizing CARP and CBASS 
solution paths an easy process. In the [Getting Started](Getting_Started.html) 
vignettee we provide a quick start guide for basic usage, fitting, 
and plotting. In this vignette, we build on the basics and provide a more 
detailed explanation for the variety of options available in `clustRviz`.



## Background

The starting point for CARP the Convex Clustering problem. 


$$
\underset{\vec U}{\textrm{minimize}} \;\;
\frac{1}{2} \| \vec X - \vec U \|_F^2 + 
\lambda \sum_{ l < m} 
w_{l,m} \| \vec u_l - \vec u_m \|_2
$$

The above can be solved via ADMM

$$
\underset{\vec U, \vec V}{\text{minimize}}\;\;
\frac{1}{2} \| \vec X - \vec U \|_2^2  +  \lambda \sum_{l<m} w_{l,m} \|  \vec v_{l,m} \|_2 \\
\text{subject to}\;\;\;\;\;
\vec u_{l} - \vec u_{m} = \vec v_{l,m}, \;\; \forall l,m
$$

Convex clustering is traditionally solved by iteratively applying ADMM updates until convergence.

This must be done for each $\lambda_k$ which is costly

CARP augments the ADMM as follows:


## Preprocessing and Inputs

While the `CARP` and `CBASS` functions provides several reasonable default choices for weights, algorithms, etc, it
is important to know their details if one wishes to compute more customized clustering choices.
Here we examine several of the inputs to `CARP` and `CBASS`, as well as their preprocessing technqiues 

While we will still use the `CARP` and `CBASS` functions, we will also encounter serveral other
package functions along the way.

As before, we will use the presidential speech dataset to illustrate 
the variety of options available. Let's begin by loading our
package and the dataset.

```{r}
library(clustRviz)
data("presidential_speech")
Xdat <- presidential_speech$X
Xdat[1:5,1:5]
```

Again our speech data consists of log-transformed word counts from speech found at url.

### Preprocessing 

##### Normalization
An important first choice before clustering is whether to center and scale our observations.

Centering should always be done.

The choice of scaling is left to the user discression, but should typically be applied 
if measurements are a vastly different scales.

In the case of the presidental speech dataset, all variables are of the same type and 
so we do not scale our data matrix

```{r}
# Centering data before computing the CARP solution path
Xdat.preprocessed <- scale(Xdat,center=TRUE,scale=FALSE)
```

In the `CARP` function this preprocessing is done via the `X.center` and `X.scale`
arguements.
If the data is pre-processed outside of `CARP`, as is the case here, these options
may be set to `FALSE`

Similarly, the `CBASS` function also requires that data preprocessed prior usage.
Because `CBASS` clusters both observations and variables, here centering is done 
by subtracting away the global mean of our data matrix.

```{r}
# Subtracting global mean before computing the CBASS solution path
Xdat.bi <- Xdat
Xdat.bi.preprocessed <- Xdat -  mean(Xdat)
```

##### Dimension Reduction

While not directly addressed by `CARP` or `CBASS`, high dimensional 
measurements can present a challenge for clustering methods.

Owing to the "curse of dimensionality", high dimensional measurements 
may deliver sub-optimal performance for distance-based methods generally.

As such, performing dimensionality reduction before applying `CARP`  
may result in more interpretable clusters.

We leave the choice of dimensionality reduction to the end-user, 
but still recommend the reduced feature set be pre-processed 
as described above.

For the purpose of visualuization, `CARP` addresses the problem 
of high dimenstionality by visualizing the principal components 
of the data by default.


### Weights

Weights are arguably one of the most important inputs to both `CARP` and `CBASS`.

As we can see from the optimization probelms above, weights impart apriori 
preference concerning which observations (and variables in the case of `CBASS`)
should be fused together.

This important feature allows us to incorportate domain knowledge, if available.

In the absense of domain knowledge, however, weights can easily be 
constructed via the data itself.

While perhaps not appearent at first, weight choices can also greatly 
affect the computation time of both `CARP` and `CBASS`.

In the `CARP` problem above, for example, we note the regularization term 
contains $\binom{n}{2}$ summands, making computations slow.

Computations can be reduced by making the majority of weights zero and 
greatly reducing compuatation time.

Here we discuss `clustRviz`'s default weight choices, as well as how 
weights can be constructed manually.

By default `CARP` and `CBASS` assume no prior domain knowledge 
and construct distance-based weights via a gaussian kernel:

$$
w_{l,m}  = \exp\{ -\phi \|\vec x_l - \vec x_m \|^2_2 \}
$$
Given a choice of $\phi$ (discussed in more detail below), an initial set of 
weights can be constructed via `DenseWeights` function. In the case of presidential
speech data for example,

```{r}
dense.weights <- DenseWeights(X = Xdat.preprocessed,phi=1e-3)
head(dense.weights)
```

As the name suggests `DenseWeights` computes $w_{l,m}$ for all pairs of observations.

For the presidents data, with $n=44$ observations, this results in a vector of length 
$\binom{44}{2} = 956$.

```{r}
length(dense.weights)
```

The weights are returned in lexicographical order, namely 
$\vec w = (w_{1,2},w_{1,3},\dots,w_{l,m},\dots,w_{n-1,n} )$

The value of $\phi$ can have a marked effect on the weights.

```{r, fig.height=5,fig.width=10,fig.align='center',echo=FALSE,fig.cap="Dense weights for different choices of phi"}
library(tidyverse)
gridExtra::grid.arrange(
DenseWeights(X = Xdat.preprocessed,phi=1e-3) %>%
  qplot(bins=35)+
  xlab('Weight Value')+
  ylab('Count'),
DenseWeights(X = Xdat.preprocessed,phi=1e-1) %>%
  qplot(bins=35) +
  xlab('Weight Value')+
  ylab('Count'),
ncol=2
)
```

The choice of $\phi$ determines variance of the weights.

Generally, choices of $\phi$ which result in higher variance weights tend to be more 
informative. For example
```{r}
phi.vec <- 10^(-4:4)
sapply(phi.vec,function(phi){
  var(DenseWeights(X=Xdat.preprocessed,phi = phi))
}) %>%
  which.max() %>%
  phi.vec[.]
```

Additionally, `DenseWeights` allows for the specification of alternative distance measures

```{r}
dense.weights.can <- DenseWeights(X = Xdat.preprocessed,phi=1e-5,method = 'canberra')
```

```{r, fig.height=5,fig.width=10,fig.align='center',echo=FALSE,fig.cap="Dense weights with Canberra distance"}
qplot(dense.weights.can,bins=35) 
```
## Fitting



## Visualizations




