<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Computational Details of the CARP and CBASS Algorithms • clustRviz</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Computational Details of the CARP and CBASS Algorithms">
<meta property="og:description" content="clustRviz">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">clustRviz</a>
        <span class="version label label-info" data-toggle="tooltip" data-placement="bottom" title="clustRviz is not yet on CRAN">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/clustRviz.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Tutorial.html">10-Minute Introduction to ClustRviz</a>
    </li>
    <li>
      <a href="../articles/Weights.html">Weight Selection for Convex Clustering and BiClustering</a>
    </li>
    <li>
      <a href="../articles/Algorithms.html">Computational Details of the CARP and CBASS Algorithms</a>
    </li>
    <li>
      <a href="../articles/Acknowledgements.html">Acknowledgements</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/DataSlingers/clustRviz/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="Algorithms_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Computational Details of the CARP and CBASS Algorithms</h1>
                        <h4 class="author">Michael Weylandt</h4>
            <address class="author_afil">
      Department of Statistics, Rice University<br><a class="author_email" href="mailto:#"></a><a href="mailto:michael.weylandt@rice.edu" class="email">michael.weylandt@rice.edu</a>
      </address>
                              <h4 class="author">John Nagorski</h4>
            <address class="author_afil">
      Department of Statistics, Rice University<br><h4 class="author">Genevera I. Allen</h4>
            <address class="author_afil">
      <div class="line-block">Departments of Statistics, Computer Science, and Electical and Computer Engineering, Rice University<br>
      Jan and Dan Duncan Neurological Research Institute, Baylor College of Medicine</div>
<br><a class="author_email" href="mailto:#"></a><a href="mailto:gallen@rice.edu" class="email">gallen@rice.edu</a>
      </address>
                  
            <h4 class="date">Last Updated: July 20th, 2020</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/DataSlingers/clustRviz/blob/master/vignettes/Algorithms.Rmd"><code>vignettes/Algorithms.Rmd</code></a></small>
      <div class="hidden name"><code>Algorithms.Rmd</code></div>

    </address>
</div>

    
    
<p>In this vignette, we give an overview of the <code>CARP</code> and <code>CBASS</code> algorithms. For more details, see Weylandt, Nagorski, and Allen <span class="citation">(2020)</span> and Weylandt <span class="citation">(2019)</span>.</p>
<div id="convex-clustering" class="section level2">
<h2 class="hasAnchor">
<a href="#convex-clustering" class="anchor"></a>Convex Clustering</h2>
<p><code>CARP</code> begins with the convex clustering problem originally popularized by Hocking <em>et al.</em> <span class="citation">(2011)</span>:<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\sum_{(i, j) \in \mathcal{E}} \|U_{i\cdot} - U_{j\cdot}\|_q\]</span></p>
<p>Note that the second term can be written as <span class="math inline">\(\|DU\|_{q, 1} = \sum_l \|(DU)_{l\cdot}\|_q\)</span> where</p>
<p><span class="math display">\[D_{l\cdot} \text{ is a vector of zeros except having a 1 where edge $l$ starts and a $-1$ where it ends} \]</span></p>
<p>giving the problem</p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\|DU\|_q\]</span></p>
<p>As noted by Chi and Lange <span class="citation">(2015)</span>, this formulation suggests the use of an operator splitting method. We consider an ADMM algorithm <span class="citation">(Boyd et al. 2011)</span>, beginning by introducing a copy variable <span class="math inline">\(V = DU\)</span> to reformulate the problem as:</p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\|V\|_{q, 1} \text{ subject to } DU - V = 0\]</span></p>
<p>In our experiments, we have found that working in matrix notation, rather than the vectorized approach of Chi and Lange <span class="citation">(2015)</span>, yields code which is faster as well as more easily maintained.</p>
<p>We then analyze this problem in a matrix analogue of the scaled form ADMM presented in Section 3.1.1 of Boyd <em>et al</em> <span class="citation">(2011)</span>:</p>
<p><span class="math display">\[\begin{align*}
U^{(k + 1)} &amp;= \text{arg min}_U \frac{1}{2}\|U - X\|_F^2 + \frac{\rho}{2}\|DU - V^{(k)} + Z^{(k)}\|_F^2 \\
V^{(k + 1)} &amp;= \text{arg min}_V \lambda\|V\|_{q, 1} + \frac{\rho}{2}\|DU^{(k + 1)} - V + Z^{(k)}\|_F^2 \\
Z^{(k + 1)} &amp;= Z^{(k)} + DU^{(k+1)} - V^{(k+1)}
\end{align*}\]</span></p>
<p>Note that our matrix variables <span class="math inline">\(U, V, Z\)</span> correspond to Boyd <em>et al.</em>’s vector variables <span class="math inline">\(x, z, u\)</span>.</p>
<p>The first problem can be solved exactly by relatively simple algebra. We note that the Frobenius norm terms can be combined to express the problem as <span class="math display">\[\begin{align*}
\text{arg min}_U &amp; \frac{1}{2}\|U - X\|_F^2 + \frac{1}{2}\|\sqrt{\rho} * (DU - V^{(k)} + Z^{(k)})\|_F^2 \\
\text{arg min}_U &amp; \frac{1}{2}\left\|\begin{pmatrix} I \\ \sqrt{\rho}D\end{pmatrix} U - \begin{pmatrix} X \\ \sqrt{\rho}(V^{(k)} - Z^{(k)}) \end{pmatrix}\right\|_F^2
\end{align*}\]</span></p>
<p>This latter term is essentially a multi-response (ridge) regression problem and has an analytical solution given by: <span class="math display">\[\left(\begin{pmatrix} I \\ \sqrt{\rho}D \end{pmatrix}^T\begin{pmatrix} I \\ \sqrt{\rho}D \end{pmatrix}\right)^{-1}\begin{pmatrix} I \\ \sqrt{\rho}D \end{pmatrix}^T\begin{pmatrix} X \\ \sqrt{\rho}(V^{(k)} - Z^{(k)}) \end{pmatrix} = \left(I + \rho D^TD\right)^{-1}\left[X + \rho D^T\left(V^{(k)} - Z^{(k)}\right)\right]\]</span></p>
<p>Next, we note that the <span class="math inline">\(V^{(k)}\)</span> can be expressed in terms of a proximal operator: <span class="math display">\[\text{arg min}_V \lambda \|V\|_{q, 1} + \frac{\rho}{2}\|DU^{(k + 1)} - V + Z^{(k)}\|_F^2 = \textsf{prox}_{\|\cdot\|_{q, 1} * \lambda/\rho}(DU^{(k + 1)} + Z^{(k)})\]</span> where the matrix norm <span class="math inline">\(\|\cdot\|_{q, 1}\)</span> is the sum of the <span class="math inline">\(\ell_q\)</span>-norm of each row. Since this norm is separable across rows, evaluation of the overall proximal operator can be reduced to evaluation of the proximal operator of the <span class="math inline">\(\ell_q\)</span>-norm.</p>
<p><code>clustRviz</code> currently only supports the <span class="math inline">\(q = 1, 2\)</span> cases, which have closed form solutions: <span class="math display">\[V^{(k +1)}_{ij} = \textsf{SoftThresh}_{\lambda/\rho}\left((DU^{(k+1)} + Z^{(k)})_{ij}\right) \text{ when } q = 1\]</span> and <span class="math display">\[V^{(k +1)}_{i\cdot} = \left(1 - \frac{\lambda}{\rho \|(DU^{(k + 1)} + Z^{(k)})_{i\cdot}\|_2}\right)_+(DU^{(k + 1)} + Z^{(k)})_{i\cdot}\text{ when } q = 2\]</span></p>
<p>The <span class="math inline">\(Z^{(k)}\)</span> update is trivial.</p>
<p>The combined algorithm is thus given by: <span class="math display">\[\begin{align*}
U^{(k + 1)} &amp;= (I + \rho D^TD)^{-1}\left[X + \rho D^T*(V^{(k)} - Z^{(k)})\right]\\
V^{(k + 1)} &amp;= \textsf{SoftThresh}_{\lambda / \rho}((DU^{(k + 1)} + Z^{(k)})) \\
Z^{(k + 1)} &amp;= Z^{(k)} + DU^{(k +1)} - V^{(k + 1)}
\end{align*}\]</span> in the <span class="math inline">\(\ell_1\)</span> case and <span class="math display">\[\begin{align*}
U^{(k + 1)} &amp;= (I + \rho D^TD)^{-1}\left[X + \rho D^T*(V^{(k)} - Z^{(k)})\right]\\
V^{(k + 1)}_{i\cdot} &amp;= \left(1 - \frac{\lambda}{\rho \|(DU^{(k + 1)} + Z^{(k)})_{i\cdot}\|_2}\right)_+(DU^{(k + 1)} + Z^{(k)})_{i\cdot} \qquad \text{ for each } i \\
Z^{(k + 1)} &amp;= Z^{(k)} + DU^{(k +1)} - V^{(k + 1)}
\end{align*}\]</span> in the <span class="math inline">\(\ell_2\)</span> case.</p>
<p>In practice, we pre-compute a Cholesky factorization of <span class="math inline">\(I + \rho D^TD\)</span> which can be used in each <span class="math inline">\(U\)</span> update.</p>
<p>We use these updates in an algorithmic regularization scheme, as described in Hu, Chi, and Allen <span class="citation">(2016)</span> to obtain the standard (non-backtracking) <code>CARP</code> algorithm:</p>
<ul>
<li>Input:
<ul>
<li>Data matrix: <span class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span>
</li>
<li>Weighted edge set: <span class="math inline">\(\mathcal{E} = \{(e_l, w_l)\}\)</span>
</li>
<li>Relaxation parameter: <span class="math inline">\(\rho \in \mathbb{R}_{&gt; 0}\)</span>
</li>
</ul>
</li>
<li>Precompute:
<ul>
<li>Difference matrix <span class="math inline">\(D \in \mathbb{R}^{|\mathcal{E}| \times n}\)</span>
</li>
<li>Cholesky factor <span class="math inline">\(L = \textsf{chol}(I + \rho D^TD) \in \mathbb{R}^{n \times n}\)</span>
</li>
</ul>
</li>
<li>Initialize:
<ul>
<li>
<span class="math inline">\(U^{(0)} = X\)</span>, <span class="math inline">\(V^{(0)} = DX\)</span>, <span class="math inline">\(Z^{(0)} = V^{(0)}\)</span>, <span class="math inline">\(\gamma^{(1)} = \epsilon\)</span>, <span class="math inline">\(k = 1\)</span>
</li>
</ul>
</li>
<li>Repeat until <span class="math inline">\(\|V^{(k - 1)}\| = 0\)</span>:
<ul>
<li><span class="math inline">\(U^{(k)} = L^{-T}L^{-1}\left[X + \rho D^T(V^{(k - 1)} - Z^{(k - 1)})\right]\)</span></li>
<li>If <span class="math inline">\(q = 1\)</span>, for all <span class="math inline">\((i, j)\)</span>: <span class="math display">\[V_{ij}^{(k)} = \textsf{SoftThreshold}_{w_i \gamma^{(k)}/ \rho}((DU^{(k)} + Z^{(k - 1)})_{ij})\]</span>
</li>
<li>If <span class="math inline">\(q = 2\)</span>, for all <span class="math inline">\(l\)</span>: <span class="math display">\[V^{(k)}_{l\cdot} = \left(1 - \frac{\gamma^{(k)} w_l}{\rho\|(DU^{(k)} + Z^{(k - 1)})_{l\cdot}\|_2}\right)_+(DU^{(k)} + Z^{(k - 1)})_{l\cdot}\]</span>
</li>
<li><span class="math inline">\(Z^{(k)} = Z^{(k - 1)} + DU^{(k)} - V^{(k)}\)</span></li>
<li><span class="math inline">\(\gamma^{(k + 1)} = t \gamma^{(k)}\)</span></li>
<li><span class="math inline">\(k := k + 1\)</span></li>
</ul>
</li>
<li>Return <span class="math inline">\(\{(U^{(l)}, V^{(l)}\}_{l = 0}^{k - 1}\)</span>
</li>
</ul>
<p>In <code>clustRviz</code>, we do not return the <span class="math inline">\(Z^{(k)}\)</span> iterates, but we do return the <span class="math inline">\(U^{(k)}\)</span> and <span class="math inline">\(V^{(k)}\)</span> iterates, as well as the zero pattern of the latter (which is useful for identifying clusters and forming dendrograms).</p>
<div id="missing-data-support" class="section level3">
<h3 class="hasAnchor">
<a href="#missing-data-support" class="anchor"></a>Missing Data Support</h3>
<p>In some applications, it is important to allow for missing data in the data matrix $X. While it is possible to use convex clustering inside of a standard multiple imputation scheme, it is also possible to perform simultaneous imputation and clustering through a minor modification of the standard convex clustering problem. In particular, we omit the unobserved (missing) values from the Frobenius norm loss (data fidelity term): <span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|\mathcal{P}_M(U - X)\|_F^2 + \lambda\|DU\|_q\]</span> where <span class="math inline">\(\mathcal{P}_M(\cdot)\)</span> is a masking operator according to the matrix <span class="math inline">\(M\)</span>; that is, <span class="math inline">\(\mathcal{P}_M(X)_{ij}\)</span> is <span class="math inline">\(X_{ij}\)</span> is <span class="math inline">\(M_{ij}\)</span> is 1 and 0 if <span class="math inline">\(M_{ij}\)</span> is 0.</p>
<p>Plugging this into the ADMM derived above, we see that the primal update requires solving the following stationarity condition: <span class="math display">\[0 = M \odot (U - X) + \rho D^TDU + \rho D^T(Z^{(k)} - V^{(k)}).\]</span> This theoretically admits an analytical update, <span class="math display">\[U^{(k+1)} = \text{unvec}\left[\left(\text{diag}(\text{vec}(M)) + I \otimes (\rho D^TD)\right)^+\text{vec}\left(M\odot X + \rho D^T(V^{(k)} - Z^{(k)})\right)\right]\]</span> where <span class="math inline">\(A^+\)</span> is the Moore-Penrose pseudo-inverse of <span class="math inline">\(A\)</span>, but is unweildy and inefficient in practice.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>To avoid this, we instead use a <em>Generalized</em> ADMM scheme in the sense of Deng and Yin <span class="citation">(2016)</span>, where we augment the <span class="math inline">\(U\)</span>-subproblem with a positive-semi-definite quadratic operator applied to <span class="math inline">\(U - U^{(k)}\)</span>: that is, instead of solving the standard ADMM update, <span class="math display">\[\text{arg min}_{U \in \mathbb{R}^{n \times p}} \frac{1}{2}\left\|\mathcal{P}_{M}(U - X)\right\|_{F}^2 + \frac{\rho}{2}\left\|DU - V^{(k)} + Z^{(k)}\right\|_F^2,\]</span> we solve the modified update, <span class="math display">\[\text{arg min}_{U \in \mathbb{R}^{n \times p}} \frac{1}{2}\left\|\mathcal{P}_{M}(U - X)\right\|_{F}^2 + \frac{\rho}{2}\left\|DU - V^{(k)} + Z^{(k)}\right\|_F^2 + \mathfrak{Q}(U - U^{(k)})\]</span> for some quadratic <span class="math inline">\(\mathfrak{Q}\)</span>. If we take <span class="math display">\[\mathfrak{Q}(U - U^{(k)}) = \frac{1}{2}\left\|\mathcal{P}_{I - M}(U - U^{(k)})\right\|_F^2\]</span> the stationarity conditions become <span class="math display">\[0 = M \odot (U - X) + \rho D^TD U + \rho D^T(Z^{(k)} - V^{(k)}) + (I - M)\odot (U - U^{(k)})\]</span> which has the analytical solution: <span class="math display">\[U^{(k+1)} = (I + \rho D^TD)^{-1}\left(M \odot X + (I - M) \odot U^{(k)} + \rho D^T(V^{(k)} - Z^{(k)})\right).\]</span> As noted above, by caching the Cholesky factorization of <span class="math inline">\((I + \rho D^TD)\)</span> we can reduce the per iteration cost. We note that this modified update is quite straight-forward and admits a simple interpretation: at each iteration, the missing elements of <span class="math inline">\(X\)</span> are imputed using the <em>previous</em> values of <span class="math inline">\(U\)</span>. In the case of no missing data, this simplifies to the updates derived above.</p>
</div>
</div>
<div id="convex-bi-clustering" class="section level2">
<h2 class="hasAnchor">
<a href="#convex-bi-clustering" class="anchor"></a>Convex Bi-Clustering</h2>
<p><code>CBASS</code> begins with the convex biclustering problem originally posed by Chi, Allen, and Baraniuk <span class="citation">(2017)</span>:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\left(\sum_{(i, j) \in \mathcal{E_1}} \|U_{i\cdot} - U_{j\cdot}\|_q + \sum_{(k, l) \in \mathcal{E_2}}\|U_{\cdot k} - U_{\cdot l}\|_q\right)\]</span></p>
<p>As before, we simplify notation by introducing two difference matrices <span class="math inline">\(D_{\text{row}}, D_{\text{col}}\)</span> to write the problem as:</p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\left(\|D_{\text{row}}U\|_{q, 1} + \|UD_{\text{col}}\|_{1, q}\right)\]</span></p>
<p>Weylandt <span class="citation">(2019)</span> considers several approaches to solving this problem and finds that a Generalized ADMM <span class="citation">(Deng and Yin 2016)</span> performs the best. By casting the problem in terms of compound copy and dual variables (<span class="math inline">\((V_{\text{row}}, V_{\text{col}})\)</span> and <span class="math inline">\((Z_{\text{row}}, Z_{\text{col}})\)</span> respectively), the updates separate “block-wise” yielding the following ADMM updates <span class="math display">\[\begin{align*}
V^{(k+1)}_{\text{row}} &amp;= \textsf{prox}_{\|\cdot\|_{\text{row}, q} * \lambda/\rho}(D_{\text{row}}U^{(k + 1)} + Z^{(k)}_{\text{row}}) \\
V^{(k+1)}_{\text{col}} &amp;= \textsf{prox}_{\|\cdot\|_{\text{col}, q} * \lambda/\rho}(U^{(k + 1)}D_{\text{col}} + Z^{(k)}_{\text{col}}) \\
Z^{(k+1)}_{\text{row}} &amp;= Z^{(k)}_{\text{row}} + D_{\text{row}}U^{(k+1)} - V^{(k+1)}_{\text{row}} \\
Z^{(k+1)}_{\text{col}} &amp;= Z^{(k)}_{\text{col}} + U^{(k+1)}D_{\text{col}} - V^{(k+1)}_{\text{col}}
\end{align*}\]</span> where the proximal operators are given by row- and column-wise element-wise or group-wise soft-thresholding for <span class="math inline">\(q = 1, 2\)</span> respectively. See Appendix B of Weylandt <span class="citation">(2019)</span> for a more detailed derivation.</p>
<p>The primal (<span class="math inline">\(U\)</span>) update is more complicated: the naive update <span class="math display">\[U^{(k+1)} = \text{arg min}_{U \in \mathbb{R}^{n \times p}} \frac{1}{2} \left\|U - X\right\|_F^2 + \frac{\rho}{2}\left\|D_{\text{row}}U - V^{(k)}_{\text{row}} + Z^{(k)}_{\text{row}}\right\|_F^2 + \frac{\rho}{2}\left\|UD_{\text{col}} - V^{(k)}_{\text{col}} + Z^{(k)}_{\text{col}}\right\|_F^2\]</span> yields the following stationary condition: <span class="math display">\[O = U - X + \rho\left(D_{\text{row}}^T(D_{\text{row}}U - V^{(k)}_{\text{row}} + Z^{(k)}_{\text{row}})\right) + \rho\left((UD_{\text{col}} - V^{(k)}_{\text{col}} + Z^{(k)}_{\text{col}})D_{\text{col}}^T\right).\]</span> Solving this directly requires solving a Sylvester equation in <span class="math inline">\(U\)</span>.</p>
<p>To avoid the expensive Sylvester step, we augment the primal problem with the positive-definite quadratic operator <span class="math display">\[\mathfrak{Q}(U - U^{(k)}) = \frac{\alpha}{2}\left\|(U - U^{(k)})\right\|_F^2 - \frac{\rho}{2}\left\|D_{\text{row}}U - D_{\text{row}}U^{(k)}\right\|_F^2 - \frac{\rho}{2}\left\|UD_{\text{col}} - U^{(k)}D_{\text{col}}\right\|_F^2.\]</span> Solving the associated stationary conditions gives the update <span class="math display">\[U^{(k+1)} = \frac{\alpha U^{(k)} + X + \rho D_{\text{row}}^T(V^{(k)}_{\text{row}} - Z^{(k)}_{\text{row}} - D_{\text{row}}U^{(k)}) + \rho (V^{(k)}_{\text{col}} - Z^{(k)}_{\text{col}} - U^{(k)}D_{\text{col}})D_{\text{col}}^T}{1 + \alpha}\]</span> where <span class="math inline">\(\alpha\)</span> is chosen sufficiently large such that <span class="math inline">\(\mathfrak{Q}\)</span> is positive-definite. <code>CBASS</code> uses a loose upper-bound of twice the sum of the maximum degrees of the row- and column-weight graphs (<em>i.e.</em>, twice the sum of the row- and column-wise <span class="math inline">\(\ell_{\infty, 1}\)</span> norms of <span class="math inline">\(D_{\text{row}}\)</span> and <span class="math inline">\(D_{\text{col}}\)</span>). See Appendix A of Weylandt <span class="citation">(2019)</span> for details.</p>
<p>Putting this all together in an algorithmic regularization scheme <span class="citation">(Hu, Chi, and Allen 2016)</span>, we obtain the standard (non-backtracking) <code>CBASS</code> algorithm:</p>
<ul>
<li>Input:
<ul>
<li>Data Matrix: <span class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span>
</li>
<li>Weighted edge sets: <span class="math inline">\(\mathcal{E}_{\text{row}} = \{(e_l^{\text{row}}, w_l^{\text{row}})\}\)</span> and <span class="math inline">\(\mathcal{E}_{\text{col}} = \{(e_l^{\text{col}}, w_l^{\text{col}})\}\)</span>
</li>
<li>Relaxation Parameter: <span class="math inline">\(\rho \in \mathbb{R}_{&gt; 0}\)</span>
</li>
</ul>
</li>
<li>Precompute:
<ul>
<li>Difference matrices: <span class="math inline">\(D_{\text{row}} \in \mathbb{R}^{|\mathcal{E}_{\text{row}}| \times n}\)</span> and <span class="math inline">\(D_{\text{col}} \in \mathbb{R}^{p \times |\mathcal{E}_{\text{col}}|}\)</span>
</li>
<li>Generalized ADMM parameter: <span class="math inline">\(\alpha \in \mathbb{R}_{&gt; 0}\)</span>
</li>
</ul>
</li>
<li>Initialize:
<ul>
<li>
<span class="math inline">\(U^{(0)} = X\)</span>, <span class="math inline">\(V^{(0)}_{\text{row}} = D_{\text{row}}X\)</span>, <span class="math inline">\(Z^{(0)}_{\text{row}} = V^{(0)}_{\text{row}}\)</span>, <span class="math inline">\(V^{(0)}_{\text{col}} = XD_{\text{col}}X\)</span>, <span class="math inline">\(Z^{(0)}_{\text{col}} = V^{(0)}_{\text{col}}\)</span>, <span class="math inline">\(\gamma^{(1)} = \epsilon\)</span>, <span class="math inline">\(k = 1\)</span>
</li>
</ul>
</li>
<li>Repeat until <span class="math inline">\(\|V^{(k - 1)}_{\text{row}}\| = \|V^{(k-1)}_{\text{col}}\|= 0\)</span>:
<ul>
<li><span class="math inline">\(U^{(k)} = \frac{\alpha U^{(k-1)} + X + \rho D_{\text{row}}^T(V^{(k-1)}_{\text{row}} - Z^{(k-1)}_{\text{row}} - D_{\text{row}}U^{(k-1)}) + \rho (V^{(k-1)}_{\text{col}} - Z^{(k-1)}_{\text{col}} - U^{(k-1)}D_{\text{col}})D_{\text{col}}^T}{1 + \alpha}\)</span></li>
<li>If <span class="math inline">\(q = 1\)</span>, for all <span class="math inline">\((i, j)\)</span>: <span class="math display">\[(V^{(k)}_{\text{row}})_{ij} = \textsf{SoftThreshold}_{w_i^{\text{row}} \gamma^{(k)}/ \rho}((D_{\text{row}}U^{(k)} + Z^{(k - 1)}_{\text{row}})_{ij})\]</span> <span class="math display">\[(V^{(k)}_{\text{col}})_{ij} = \textsf{SoftThreshold}_{w_j^{\text{col}} \gamma^{(k)}/ \rho}((U^{(k)}D_{\text{col}} + Z^{(k - 1)}_{\text{col}})_{ij})\]</span>
</li>
<li>If <span class="math inline">\(q = 2\)</span>, for all <span class="math inline">\(l\)</span>: <span class="math display">\[(V^{(k)}_{\text{row}})_{l\cdot} = \left(1 - \frac{\gamma^{(k)} w_l^{\text{row}}}{\rho\|(D_{\text{row}}U^{(k)} + Z^{(k - 1)}_{\text{row}})_{l\cdot}\|_2}\right)_+(D_{\text{row}}U^{(k)} + Z^{(k - 1)}_{\text{row}})_{l\cdot}\]</span> <span class="math display">\[(V^{(k)}_{\text{col}})_{\cdot l} = \left(1 - \frac{\gamma^{(k)} w_l^{\text{col}}}{\rho\|(U^{(k)}D_{\text{col}} + Z^{(k - 1)}_{\text{col}})_{l\cdot}\|_2}\right)_+(D_{\text{col}}U^{(k)} + Z^{(k - 1)}_{\text{col}})_{l\cdot}\]</span>
</li>
<li><span class="math inline">\(Z^{(k)}_{\text{row}} = Z^{(k - 1)}_{\text{row}} + D_{\text{row}}U^{(k)} - V^{(k)}_{\text{row}}\)</span></li>
<li><span class="math inline">\(Z^{(k)}_{\text{col}} = Z^{(k - 1)}_{\text{col}} + U^{(k)}D_{\text{col}} - V^{(k)}_{\text{col}}\)</span></li>
<li><span class="math inline">\(\gamma^{(k + 1)} = t \gamma^{(k)}\)</span></li>
<li><span class="math inline">\(k := k + 1\)</span></li>
</ul>
</li>
<li>Return <span class="math inline">\(\{(U^{(l)}, V^{(l)}_{\text{row}}, V^{(l)}_{\text{col}}\}_{l = 0}^{k - 1}\)</span>
</li>
</ul>
<p>As in <code>clustRviz</code>, we do not return the <span class="math inline">\(Z^{(k)}_{\text{row}}\)</span> or <span class="math inline">\(Z^{(k)}_{\text{col}}\)</span> iterates, but we do return the<span class="math inline">\(U^{(k)}\)</span>, <span class="math inline">\(V^{(k)}_{\text{row}}\)</span>, <span class="math inline">\(V^{(k)}_{\text{col}}\)</span> iterates, as well as the zero pattern of the latter (which is useful for identifying biclusters and forming row and column dendrograms).</p>
<p>Note that the biclustering objective can be interpreted as the proximal operator of the function <span class="math inline">\(f(U) = \|D_{\text{row}}U\|_{q, 1} + \|UD_{\text{col}}\|_{1, q}\)</span>. Despite the simplicity of the proximal operators of the individual terms in <span class="math inline">\(f\)</span>, the proximal operator of the sum cannot be computed explicitly. To address this difficulty, use the Dykstra-Like Proximal Algorithm (DLPA) of Bauschke and Combettes <span class="citation">(2008; see also Combettes and Pesquet 2011)</span> which allows us to evaluate the proximal operator of the sum by repeated evaluation of the proximal operators of the summands. A modified version of the DLPA was used as the basis of an algorithmic regularization scheme of the sort described above in a previous version of <code>CBASS</code>. For details, see Appendix C of Weylandt, Nagorski, and Allen <span class="citation">(2020)</span>.</p>
<div id="missing-data-support-1" class="section level3">
<h3 class="hasAnchor">
<a href="#missing-data-support-1" class="anchor"></a>Missing Data Support</h3>
<p>As with <code>CARP</code> missing data support can be added by modifying the objective function and using a quadratic pertubation of the primal update. In particular, we add a data mask to the loss function to obtain the new objective function: <span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|\mathcal{P}_M(U - X)\|_F^2 + \lambda\left(\|D_{\text{row}}U\|_{q, 1} + \|UD_{\text{col}}\|_{1, q}\right)\]</span></p>
<p>As before, we add a “complementary mask” to the quadratic term to obtain the new generalized ADMM update with <span class="math display">\[\mathfrak{Q}(U - U^{(k)}) = \frac{\alpha}{2}\left\|(U - U^{(k)})\right\|_F^2 - \frac{\rho}{2}\left\|D_{\text{row}}U - D_{\text{row}}U^{(k)}\right\|_F^2 - \frac{\rho}{2}\left\|UD_{\text{col}} - U^{(k)}D_{\text{col}}\right\|_F^2 + \frac{1}{2}\left\|\mathcal{P}_{I - M}(U - U^{(k)})\right\|_F^2.\]</span> As before, this leads to an “imputed <span class="math inline">\(X\)</span>” in the <span class="math inline">\(U\)</span>-update step of the ADMM: <span class="math display">\[U^{(k+1)} = \frac{\alpha U^{(k)} + (M \odot X + (I - M) \odot U^{(k)}) + \rho D_{\text{row}}^T(V^{(k)}_{\text{row}} - Z^{(k)}_{\text{row}} - D_{\text{row}}U^{(k)}) + \rho (V^{(k)}_{\text{col}} - Z^{(k)}_{\text{col}} - U^{(k)}D_{\text{col}})D_{\text{col}}^T}{1 + \alpha}\]</span> As before, we note that this has the “impute from previous” structure and simplififes to the standard update when there are no missing values.</p>
</div>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-Bauschke:2008">
<p>Bauschke, Heinz H., and Patrick L. Combettes. 2008. “A Dykstra-Like Algorithm for Two Monotone Operators.” <em>Pacific Journal of Optimization</em> 4 (3): 383–91.</p>
</div>
<div id="ref-Boyd:2011">
<p>Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011. “Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.” <em>Foundations and Trends in Machine Learning</em> 3 (1): 1–122. <a href="https://doi.org/10.1561/2200000016">https://doi.org/10.1561/2200000016</a>.</p>
</div>
<div id="ref-Chi:2017">
<p>Chi, Eric C., Genevera I. Allen, and Richard G. Baraniuk. 2017. “Convex Biclustering.” <em>Biometrics</em> 73 (1): 10–19. <a href="https://doi.org/10.1111/biom.12540">https://doi.org/10.1111/biom.12540</a>.</p>
</div>
<div id="ref-Chi:2015">
<p>Chi, Eric C., and Kenneth Lange. 2015. “Splitting Methods for Convex Clustering.” <em>Journal of Computational and Graphical Statistics</em> 24 (4): 994–1013. <a href="https://doi.org/10.1080/10618600.2014.948181">https://doi.org/10.1080/10618600.2014.948181</a>.</p>
</div>
<div id="ref-Combettes:2011">
<p>Combettes, Patrick L., and Jean-Cristophe Pesquet. 2011. “Proximal Splitting Methods in Signal Processing.” In <em>Fixed-Point Algorithms for Inverse Problems in Science and Engineering</em>, edited by Heinz H. Bauschke, Regina S. Burachik, Patrick L. Combettes, Veit Elser, D. Russell Luke, and Henry Wolkowicz, 185–212. Springer. <a href="https://doi.org/10.1007/978-1-4419-9569-8_10">https://doi.org/10.1007/978-1-4419-9569-8_10</a>.</p>
</div>
<div id="ref-Deng:2016">
<p>Deng, Wei, and Wotao Yin. 2016. “On the Global and Linear Convergence of the Generalized Alternating Direction Method of Multipliers.” <em>Journal of Scientific Computing</em> 66 (3): 889–916. <a href="https://doi.org/10.1007/s10915-015-0048-x">https://doi.org/10.1007/s10915-015-0048-x</a>.</p>
</div>
<div id="ref-Hocking:2011">
<p>Hocking, Toby Dylan, Armand Joulin, Francis Bach, and Jean-Philippe Vert. 2011. “Clusterpath: An Algorithm for Clustering Using Convex Fusion Penalties.” In <em>ICML 2011: Proceedings of the 28th International Conference on Machine Learning</em>, edited by Lise Getoor and Tobias Scheffer, 745–52. Bellevue, Washington, USA: ACM. <a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf">http://www.icml-2011.org/papers/419_icmlpaper.pdf</a>.</p>
</div>
<div id="ref-Hu:2016">
<p>Hu, Yue, Eric C. Chi, and Genevera I. Allen. 2016. “ADMM Algorithmic Regularization Paths for Sparse Statistical Machine Learning.” In <em>Splitting Methods in Communication and Imaging, Science, and Engineering</em>, edited by Roland Glowinski, Stanley J. Osher, and Wotao Yin, 433–49. Springer. <a href="https://doi.org/10.1007/978-3-319-41589-5_13">https://doi.org/10.1007/978-3-319-41589-5_13</a>.</p>
</div>
<div id="ref-Weylandt:2019b">
<p>Weylandt, Michael. 2019. “Splitting Methods for Convex Bi-Clustering and Co-Clustering.” In <em>DSW 2019: Proceedings of the IEEE 2019 Data Science Workshop</em>, edited by George Karypis, George Michailidis, and Rebecca Willett, 237–42. IEEE. <a href="https://doi.org/10.1109/DSW.2019.8755599">https://doi.org/10.1109/DSW.2019.8755599</a>.</p>
</div>
<div id="ref-Weylandt:2019">
<p>Weylandt, Michael, John Nagorski, and Genevera I. Allen. 2020. “Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization.” <em>Journal of Computational and Graphical Statistics</em> 29 (1): 87–96. <a href="https://doi.org/10.1080/10618600.2019.1629943">https://doi.org/10.1080/10618600.2019.1629943</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Here, we consider the case of uniform weights to simplify some of the notation, but the general case is essentially the same. The general formulation of <code>CARP</code> is given below.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>As discussed at <a href="https://scicomp.stackexchange.com/q/31001/28552" class="uri">https://scicomp.stackexchange.com/q/31001/28552</a>, this can be computed without instantiating the Kronecker product, at the cost of calculating the columns of <span class="math inline">\(U^{(k+1)}\)</span> separately.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Again, we consider the case of uniform weights to simplify some of the notation and give the general case at the end of this section.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Michael Weylandt, John Nagorski, Genevera Allen, Daniel Englund, Yue Zhuo.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
